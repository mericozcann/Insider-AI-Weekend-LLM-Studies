# -*- coding: utf-8 -*-
"""Hybrid_Fraud_Risk_Scoring_LSTM_LightGBM_SHAP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jFHe6FczV7ze0C8FM_xH_px_2O6Px6eU
"""

# =====================================================
# 0. IMPORTS & CONFIG
# =====================================================
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import (
    precision_recall_fscore_support,
    average_precision_score,
    precision_recall_curve,
    confusion_matrix
)
from sklearn.preprocessing import StandardScaler

from lightgbm import LGBMClassifier

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense
from tensorflow.keras.optimizers import Adam

import shap
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

# Veri boyutu (istersen büyüt/azalt)
N_CUSTOMERS = 1500   # daha gerçekçi: 1500 müşteri
TX_PER_CUSTOMER = 120  # müşteri başı 120 işlem -> ~180k işlem


# =====================================================
# 1. SENTETİK VE DAHA GERÇEKÇİ VERİ ÜRETİMİ
# =====================================================
def generate_synthetic_transactions(
    n_customers=N_CUSTOMERS,
    tx_per_customer=TX_PER_CUSTOMER,
    fraud_ratio=0.03,
    random_state=RANDOM_STATE
):
    rng = np.random.RandomState(random_state)
    rows = []
    base_time = datetime(2024, 1, 1)

    # Bazı müşterileri bilinçli "yüksek risk" yapıyoruz
    high_risk_customers = set(rng.choice(np.arange(n_customers), size=int(0.1 * n_customers), replace=False))

    for cust_id in range(n_customers):
        customer_risk_factor = rng.beta(2, 5)
        if cust_id in high_risk_customers:
            customer_risk_factor *= 2.0  # risk çarpanı

        last_time = base_time

        for tx_idx in range(tx_per_customer):
            # işlemler arası zaman farkı (5 dk - 12 saat)
            delta_minutes = rng.randint(5, 60 * 12)
            last_time = last_time + timedelta(minutes=int(delta_minutes))

            amount = np.round(rng.gamma(shape=2.2, scale=120), 2)  # biraz daha geniş dağılım
            channel = rng.choice(["WEB", "MOBILE", "BRANCH", "ATM"], p=[0.45, 0.4, 0.1, 0.05])
            country = rng.choice(["TR", "US", "DE", "GB", "NL"], p=[0.78, 0.08, 0.06, 0.05, 0.03])
            device_id = rng.randint(1, 2000)
            receiver_id = rng.randint(1, 10000)

            hour = last_time.hour
            day_of_week = last_time.weekday()

            # Temel fraud olasılığı: müşteri risk faktörü + davranışsal özellikler
            base_prob = 0.005 + 0.25 * customer_risk_factor

            # Gece saatleri
            if hour in [0, 1, 2, 3, 4]:
                base_prob += 0.06

            # Hafta sonu + gece kombinasyonu
            if day_of_week >= 5 and hour in [0, 1, 2, 3, 4]:
                base_prob += 0.04

            # Yüksek tutar
            if amount > 5000:
                base_prob += 0.08
            elif amount > 2000:
                base_prob += 0.04

            # Yurt dışı işlem
            if country != "TR":
                base_prob += 0.05

            # Kanal bazlı risk
            if channel == "WEB":
                base_prob += 0.01
            if channel == "MOBILE":
                base_prob += 0.01

            # global fraud_ratio'ya kabaca uyduralım
            is_fraud = rng.rand() < base_prob * (fraud_ratio / 0.05)

            rows.append({
                "customer_id": cust_id,
                "timestamp": last_time,
                "amount": amount,
                "channel": channel,
                "country": country,
                "device_id": device_id,
                "receiver_id": receiver_id,
                "hour": hour,
                "day_of_week": day_of_week,
                "is_fraud": int(is_fraud)
            })

    df = pd.DataFrame(rows)
    df = df.sort_values(["customer_id", "timestamp"]).reset_index(drop=True)
    return df


df = generate_synthetic_transactions()
print("Toplam işlem:", len(df))
print("Fraud oranı:", round(df["is_fraud"].mean(), 4))
display(df.head())


# =====================================================
# 2. FEATURE ENGINEERING
# =====================================================
def feature_engineering(df: pd.DataFrame):
    df = df.copy()

    # Kategorik değişkenleri encode et
    df["channel_enc"] = df["channel"].astype("category").cat.codes
    df["country_enc"] = df["country"].astype("category").cat.codes

    # Müşteri bazlı rolling features (velocity & davranışsal)
    df["amount_rolling_5"] = (
        df.groupby("customer_id")["amount"]
        .rolling(window=5, min_periods=1)
        .mean()
        .reset_index(level=0, drop=True)
    )

    df["amount_rolling_20"] = (
        df.groupby("customer_id")["amount"]
        .rolling(window=20, min_periods=1)
        .mean()
        .reset_index(level=0, drop=True)
    )

    # İşlem sayısı ve velocity proxy
    df["tx_index"] = df.groupby("customer_id").cumcount() + 1

    # Z-score (müşteri bazlı)
    df["amount_mean_cust"] = df.groupby("customer_id")["amount"].transform("mean")
    df["amount_std_cust"] = df.groupby("customer_id")["amount"].transform("std").fillna(1.0)
    df["amount_zscore"] = (df["amount"] - df["amount_mean_cust"]) / df["amount_std_cust"]

    # Lokasyon/device değişim flag’leri
    df["country_shift"] = df.groupby("customer_id")["country"].shift(1)
    df["device_shift"] = df.groupby("customer_id")["device_id"].shift(1)
    df["country_changed"] = (df["country"] != df["country_shift"]).astype(int)
    df["device_changed"] = (df["device_id"] != df["device_shift"]).astype(int)

    # Gün içi ve hafta içi paterni için sin/cos dönüşümleri (daha gerçekçi time features)
    df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24)
    df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24)

    df["dow_sin"] = np.sin(2 * np.pi * df["day_of_week"] / 7)
    df["dow_cos"] = np.cos(2 * np.pi * df["day_of_week"] / 7)

    df = df.fillna(0)

    feature_cols = [
        "amount",
        "hour",
        "day_of_week",
        "channel_enc",
        "country_enc",
        "amount_rolling_5",
        "amount_rolling_20",
        "tx_index",
        "amount_zscore",
        "country_changed",
        "device_changed",
        "hour_sin",
        "hour_cos",
        "dow_sin",
        "dow_cos",
    ]

    return df, feature_cols


df_fe, feature_cols = feature_engineering(df)
print("Feature sayısı:", len(feature_cols))
display(df_fe[feature_cols + ["is_fraud"]].head())


# =====================================================
# 3. LSTM AUTOENCODER İÇİN SEQUENCE HAZIRLAMA
# =====================================================
def build_sequences(df, feature_cols, window_size=50):
    """
    Her müşteri için zaman sıralı feature'ları kullanarak
    sabit uzunlukta sequence'ler oluşturur (LSTM için).
    """
    sequences = []
    seq_index = []

    for cust_id, group in df.groupby("customer_id"):
        g = group.sort_values("timestamp")
        values = g[feature_cols].values

        if len(values) < window_size:
            continue

        for i in range(len(values) - window_size + 1):
            window = values[i:i + window_size]
            sequences.append(window)
            seq_index.append({
                "customer_id": cust_id,
                "start_idx": g.index[i],
                "end_idx": g.index[i + window_size - 1]
            })

    X_seq = np.array(sequences, dtype=np.float32)
    seq_index_df = pd.DataFrame(seq_index)
    return X_seq, seq_index_df


WINDOW_SIZE = 50
X_seq, seq_index_df = build_sequences(df_fe, feature_cols, window_size=WINDOW_SIZE)
print("LSTM için sequence shape:", X_seq.shape)
display(seq_index_df.head())


# =====================================================
# 4. ZAMAN BAZLI TRAIN / VALID / TEST SPLIT
# =====================================================
t1 = df_fe["timestamp"].quantile(0.7)
t2 = df_fe["timestamp"].quantile(0.85)

train_mask = df_fe["timestamp"] <= t1
valid_mask = (df_fe["timestamp"] > t1) & (df_fe["timestamp"] <= t2)
test_mask = df_fe["timestamp"] > t2

df_train = df_fe[train_mask].copy()
df_valid = df_fe[valid_mask].copy()
df_test = df_fe[test_mask].copy()

# Sequence'leri de sequence end_time üzerinden bölelim
seq_index_df["end_time"] = df_fe.loc[seq_index_df["end_idx"], "timestamp"].values

seq_train_mask = seq_index_df["end_time"] <= t1
seq_valid_mask = (seq_index_df["end_time"] > t1) & (seq_index_df["end_time"] <= t2)
seq_test_mask = seq_index_df["end_time"] > t2

X_seq_train = X_seq[seq_train_mask.values]
X_seq_valid = X_seq[seq_valid_mask.values]
X_seq_test = X_seq[seq_test_mask.values]

print("Train seq:", X_seq_train.shape)
print("Valid seq:", X_seq_valid.shape)
print("Test  seq:", X_seq_test.shape)


# =====================================================
# 5. LSTM AUTOENCODER MODELİ (ANOMALİ TESPİTİ)
# =====================================================
def build_lstm_autoencoder(n_features, window_size, latent_dim=64, lr=0.001):
    inputs = Input(shape=(window_size, n_features))
    # Encoder
    x = LSTM(latent_dim, activation="tanh", return_sequences=False)(inputs)
    encoded = RepeatVector(window_size)(x)

    # Decoder
    x = LSTM(latent_dim, activation="tanh", return_sequences=True)(encoded)
    outputs = TimeDistributed(Dense(n_features))(x)

    model = Model(inputs, outputs)
    model.compile(optimizer=Adam(learning_rate=lr), loss="mse")
    return model


n_features = len(feature_cols)
autoencoder = build_lstm_autoencoder(
    n_features=n_features,
    window_size=WINDOW_SIZE,
    latent_dim=64,
    lr=0.001
)
autoencoder.summary()

# Demo için epoch sayısını orta düzeyde tutuyoruz
history = autoencoder.fit(
    X_seq_train, X_seq_train,
    epochs=5,
    batch_size=256,
    validation_data=(X_seq_valid, X_seq_valid),
    verbose=1
)

recon_train = autoencoder.predict(X_seq_train)
recon_valid = autoencoder.predict(X_seq_valid)
recon_test = autoencoder.predict(X_seq_test)

train_mse = np.mean(np.mean((X_seq_train - recon_train) ** 2, axis=2), axis=1)
valid_mse = np.mean(np.mean((X_seq_valid - recon_valid) ** 2, axis=2), axis=1)
test_mse = np.mean(np.mean((X_seq_test - recon_test) ** 2, axis=2), axis=1)

def min_max_norm(x):
    return (x - x.min()) / (x.max() - x.min() + 1e-6)

anomaly_score_train = min_max_norm(train_mse)
anomaly_score_valid = min_max_norm(valid_mse)
anomaly_score_test = min_max_norm(test_mse)

seq_index_df["anomaly_score"] = 0.0
seq_index_df.loc[seq_train_mask, "anomaly_score"] = anomaly_score_train
seq_index_df.loc[seq_valid_mask, "anomaly_score"] = anomaly_score_valid
seq_index_df.loc[seq_test_mask, "anomaly_score"] = anomaly_score_test

print("Sequence anomaly örneği:")
display(seq_index_df.head())


# =====================================================
# 6. SEQUENCE ANOMALY SCORE'U İŞLEM SEVİYESİNE MAP ETME
# =====================================================
df_fe["anomaly_score"] = 0.0
df_fe["anomaly_count"] = 0

for idx, row in seq_index_df.iterrows():
    start_idx = int(row["start_idx"])
    end_idx = int(row["end_idx"])
    score = row["anomaly_score"]

    df_fe.loc[start_idx:end_idx, "anomaly_score"] += score
    df_fe.loc[start_idx:end_idx, "anomaly_count"] += 1

df_fe["anomaly_score"] = df_fe["anomaly_score"] / df_fe["anomaly_count"].replace(0, 1)
df_fe["anomaly_score"] = df_fe["anomaly_score"].fillna(0.0)

print("İşlem bazında anomaly score örnekleri:")
display(df_fe[["amount", "amount_zscore", "anomaly_score", "is_fraud"]].head(10))


# =====================================================
# 7. FRAUD MODELİ İÇİN DATASET HAZIRLAMA
# =====================================================
clf_features = feature_cols + ["anomaly_score"]

X_train = df_fe.loc[train_mask, clf_features]
y_train = df_fe.loc[train_mask, "is_fraud"]

X_valid = df_fe.loc[valid_mask, clf_features]
y_valid = df_fe.loc[valid_mask, "is_fraud"]

X_test = df_fe.loc[test_mask, clf_features]
y_test = df_fe.loc[test_mask, "is_fraud"]

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled = scaler.transform(X_test)

print("Train size:", X_train.shape, " Fraud oranı:", y_train.mean())
print("Valid size:", X_valid.shape, " Fraud oranı:", y_valid.mean())
print("Test  size:", X_test.shape, " Fraud oranı:", y_test.mean())


# 8. DIRECT LIGHTGBM MODEL (NO HPO – FAST DEMO)
from lightgbm import LGBMClassifier

# Sabit ama makul bir konfigürasyon
best_model = LGBMClassifier(
    objective="binary",
    class_weight="balanced",
    n_estimators=400,
    num_leaves=63,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.85,
    colsample_bytree=0.85,
    random_state=RANDOM_STATE,
    n_jobs=-1
)

# Train + valid birleştirip final modeli eğit
X_train_full = np.vstack([X_train_scaled, X_valid_scaled])
y_train_full = np.concatenate([y_train.values, y_valid.values])

print("Final LightGBM modeli (no HPO) eğitiliyor...")
best_model.fit(X_train_full, y_train_full)
print("Model eğitimi tamamlandı.")

# =====================================================
# 9. FRAUD PROB, SKOR FÜZYONU & RISK LEVEL
# =====================================================
fraud_proba_train = best_model.predict_proba(X_train_scaled)[:, 1]
fraud_proba_valid = best_model.predict_proba(X_valid_scaled)[:, 1]
fraud_proba_test = best_model.predict_proba(X_test_scaled)[:, 1]

df_fe.loc[train_mask, "fraud_prob"] = fraud_proba_train
df_fe.loc[valid_mask, "fraud_prob"] = fraud_proba_valid
df_fe.loc[test_mask, "fraud_prob"] = fraud_proba_test

def fuse_scores(fraud_prob, anomaly_score, alpha=0.6):
    return alpha * fraud_prob + (1 - alpha) * anomaly_score

df_fe["final_risk_score"] = fuse_scores(
    df_fe["fraud_prob"].fillna(0),
    df_fe["anomaly_score"].fillna(0),
    alpha=0.6
)

def map_risk_level(score):
    if score < 0.40:
        return "LOW"
    elif score < 0.70:
        return "MEDIUM"
    else:
        return "HIGH"

df_fe["risk_level"] = df_fe["final_risk_score"].apply(map_risk_level)

print("Risk level dağılımı:")
print(df_fe["risk_level"].value_counts(normalize=True))
display(df_fe[["amount", "fraud_prob", "anomaly_score", "final_risk_score", "risk_level", "is_fraud"]].head(10))


# =====================================================
# 10. DEĞERLENDİRME METRİKLERİ (TEST SET)
# =====================================================
y_pred_proba = df_fe.loc[test_mask, "fraud_prob"].values
y_true = y_test.values

# default threshold = 0.5
y_pred_label = (y_pred_proba >= 0.5).astype(int)

precision, recall, f1, _ = precision_recall_fscore_support(
    y_true, y_pred_label, average="binary", zero_division=0
)
pr_auc = average_precision_score(y_true, y_pred_proba)
cm = confusion_matrix(y_true, y_pred_label)

print("\n--- Evaluation on Test ---")
print("Precision:", round(precision, 3))
print("Recall   :", round(recall, 3))
print("F1       :", round(f1, 3))
print("PR-AUC   :", round(pr_auc, 3))
print("Confusion matrix:\n", cm)


# =====================================================
# 11. DASHBOARD TARZI RAPORLAMA (MATPLOTLIB)
# =====================================================
fraud_rate = df_fe["is_fraud"].mean()
high_risk_rate = (df_fe["risk_level"] == "HIGH").mean()

print("\n=== Özet KPI'lar ===")
print(f"Toplam işlem: {len(df_fe):,}")
print(f"Fraud oranı: {fraud_rate:.4f}")
print(f"High risk oranı: {high_risk_rate:.4f}")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle("Fraud & Risk Dashboard (Demo)", fontsize=16)

# 1) Sınıf dağılımı
ax = axes[0, 0]
class_counts = df_fe["is_fraud"].value_counts()
ax.bar(["Non-Fraud", "Fraud"], class_counts.values)
ax.set_title("Sınıf Dağılımı (Tüm Veri)")
ax.set_ylabel("Adet")

# 2) Risk level dağılımı
ax = axes[0, 1]
risk_counts = df_fe["risk_level"].value_counts()
ax.bar(risk_counts.index, risk_counts.values)
ax.set_title("Risk Level Dağılımı")
ax.set_ylabel("Adet")

# 3) PR eğrisi (test set)
ax = axes[1, 0]
precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_proba)
ax.plot(recalls, precisions)
ax.set_title("Precision-Recall Eğrisi (Test)")
ax.set_xlabel("Recall")
ax.set_ylabel("Precision")

# 4) Final risk score histogram (test set)
ax = axes[1, 1]
ax.hist(df_fe.loc[test_mask, "final_risk_score"], bins=30)
ax.set_title("Final Risk Score Dağılımı (Test)")
ax.set_xlabel("Final Risk Score")
ax.set_ylabel("Frekans")

plt.tight_layout()
plt.show()


# =====================================================
# 12. SHAP İLE AÇIKLANABİLİRLİK
# =====================================================
background_size = min(500, X_train_full.shape[0])
test_explain_size = min(200, X_test_scaled.shape[0])

X_bg = X_train_full[:background_size]
X_explain = X_test_scaled[:test_explain_size]

explainer = shap.TreeExplainer(best_model)
shap_vals = explainer.shap_values(X_explain)

# Binary classification'da list dönebiliyor
if isinstance(shap_vals, list):
    shap_values = shap_vals[1]
else:
    shap_values = shap_vals

print("\nSHAP hesaplandı. Örnek bir test işlemi için Top-5 feature etkisi:")

instance_idx = 0
instance_shap = shap_values[instance_idx]
feature_importance = sorted(
    list(zip(clf_features, instance_shap)),
    key=lambda x: abs(x[1]),
    reverse=True
)

for feat, val in feature_importance[:5]:
    print(f"  {feat}: SHAP = {val:.4f}")

# Özet SHAP grafiğini de görebilirsin (istersen aç):
# shap.summary_plot(shap_values, X_explain, feature_names=clf_features)

# =====================================================
# 13. EN RİSKLİ İŞLEMLER (DASHBOARD TABLOSU)
# =====================================================
top_suspicious = df_fe.loc[test_mask].sort_values("final_risk_score", ascending=False).head(15)
print("\nEn riskli 15 işlem:")
display(
    top_suspicious[[
        "customer_id", "timestamp", "amount", "country", "channel",
        "fraud_prob", "anomaly_score", "final_risk_score",
        "risk_level", "is_fraud"
    ]]
)

print("\n✅ Demo proje: veri üretimi + feature engineering + LSTM anomaly + HPO + LightGBM fraud + dashboard + SHAP tamamlandı.")